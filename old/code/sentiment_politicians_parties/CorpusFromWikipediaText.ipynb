{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"wikipedia_wordclouds/\")\n",
    "sys.path.append(\"inverted_index/\")\n",
    "from wikipedia_wordclouds.wikipedia_corpus_creator import WikiCorpusCreator\n",
    "from inverted_index.inverted_index import InvertedIndex\n",
    "#import spacy\n",
    "#import pytextrank\n",
    "#from keybert import KeyBERT\n",
    "#from keyphrase_vectorizers import KeyphraseCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_POLITICIANS_PARTY_MAPPING = \"speaker_party.csv\"\n",
    "PATH_TO_ALL_TAGGED_STATEMENTS = \"all_tagged.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad75f3",
   "metadata": {},
   "source": [
    "First the two datasets are going to be joined together, so that only statements are left where the speaker and the speakers party could be added to the dataset containing the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a06ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_party_mapping = pd.read_csv(PATH_TO_POLITICIANS_PARTY_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb65d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tagged = pd.read_csv(PATH_TO_ALL_TAGGED_STATEMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_all_tagged.join(df_party_mapping.set_index('speaker'), on='speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0672d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = df_joined[~df_joined[\"party\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.to_csv(\"tagged_with_party.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7ab6c",
   "metadata": {},
   "source": [
    "### Now different approaches for creating corpora using text from Wikipedia are used to find statements regarding specific topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c086943",
   "metadata": {},
   "source": [
    "Get text from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcc = WikiCorpusCreator(\"Klimawandel\",exclude_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wcc.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3d279",
   "metadata": {},
   "source": [
    "Create the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_index = InvertedIndex(\"tagged_with_party.csv\", \"speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea05b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_index.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_documents_containing_words_in_corpus(inverted_index, word_set):\n",
    "    \n",
    "    res = []\n",
    "    results_per_word = dict()\n",
    "    \n",
    "    for word in word_set:\n",
    "        results = invert_index.search(word)\n",
    "        \n",
    "        if results is not None:\n",
    "            res.extend(results)\n",
    "            results_per_word[word] = len(results)\n",
    "        else:\n",
    "            results_per_word[word] = 0\n",
    "    \n",
    "    return set(res), results_per_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955b8aa",
   "metadata": {},
   "source": [
    "### Spacy with textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3842f27",
   "metadata": {},
   "source": [
    "execute in shell:\n",
    "```python -m spacy download de_core_news_lg```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaee258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = [\"Jahr\",\"Million\",\"Teil\",\"Milliarde\",\"Beginn\",\"Folge\",\"Zeit\",\"Sprache\",\"Bereich\",\"Beispiel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for phrase in doc._.phrases[:300]:\n",
    "#    current = phrase.text\n",
    "#    if len(current) > 3 and ' ' not in current and '„' not in current:\n",
    "#        \n",
    "#        should_be_handled = True\n",
    "#        \n",
    "#        for word in stop_words:\n",
    "#            if word in current:\n",
    "#                should_be_handled = False\n",
    "#        \n",
    "#        if should_be_handled:\n",
    "#            print(phrase.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160d29c",
   "metadata": {},
   "source": [
    "### KeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4998a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"de_core_news_lg\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kw_model = KeyBERT(model=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d995d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = kw_model.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc046f",
   "metadata": {},
   "source": [
    "### Using a vectorizer in combination with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d212ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = KeyphraseCountVectorizer(spacy_pipeline='de_core_news_lg', pos_pattern='<ADJ.*>*<N.*>+', stop_words='german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.fit([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33123ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyphrases = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for phrase in keyphrases:\n",
    "#    if ' ' not in phrase:\n",
    "#        print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278d850",
   "metadata": {},
   "source": [
    "### Individual approach using a large frequency list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142c4ab",
   "metadata": {},
   "source": [
    "https://wacky.sslmit.unibo.it/doku.php?id=frequency_lists\n",
    "\n",
    "\n",
    "\n",
    "https://www.sketchengine.eu/dewac-german-corpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87905974",
   "metadata": {},
   "source": [
    "Add an approach which is based on a frequency list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ceaf6e",
   "metadata": {},
   "source": [
    "* replaced space with tab in vim using :%s/\\s/\\t/g\n",
    "* removed the entry \" as it caused problems while reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366410b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies = pd.read_csv(\"sorted.de.word.unigrams\",encoding=\"Latin-1\",sep=\"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747398f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies.columns = [\"amount\",\"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668d024",
   "metadata": {},
   "source": [
    "Preprocessing of the unigrams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3710c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_is_word(input_string):\n",
    "    \n",
    "    if input_string.isupper():\n",
    "        return False\n",
    "    \n",
    "    return bool(re.match(\"^[A-Za-zÖÄÜöäüß]+$\",input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies[\"word\"] = df_frequencies[\"word\"].apply(lambda inp: str(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies[\"is_word\"] = df_frequencies[\"word\"].apply(lambda inp: string_is_word(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies[\"word_length\"] = df_frequencies[\"word\"].apply(lambda word: len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies[\"amount_unique_characters\"] = df_frequencies[\"word\"].apply(lambda word: len(''.join(set(word.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e12b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies_preprocessed = df_frequencies[(df_frequencies[\"is_word\"]) & \n",
    "                                             (df_frequencies[\"word_length\"] > 3) & \n",
    "                                             (df_frequencies[\"amount_unique_characters\"] > 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2fb4e",
   "metadata": {},
   "source": [
    "Filter for words that appear greater than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c48255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_filtered = df_frequencies_preprocessed[(df_frequencies_preprocessed[\"amount\"] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9121466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preprocessed_filtered[\"percentage\"] = df_preprocessed_filtered[\"amount\"] / df_preprocessed_filtered[\"amount\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a87175",
   "metadata": {},
   "source": [
    "Create a dict with percentage and word, this is used for the classification later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_percentage_dict = pd.Series(df_preprocessed_filtered.percentage.values,index=df_preprocessed_filtered.word).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_percentage = float(df_preprocessed_filtered[\"percentage\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93b87c",
   "metadata": {},
   "source": [
    "Define functions that are used for finding specific words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_words(text, word_dict, treshold, consider_words = None):\n",
    "    tokens = nltk.word_tokenize(text, language='german')\n",
    "    res = []\n",
    "    \n",
    "    # define the output of the function 'f' based on the input parameter\n",
    "    # 'consider_words'\n",
    "    if consider_words is not None:\n",
    "        def f(word):\n",
    "            return classify_word_as_specific_word(word,word_percentage_dict,mean_percentage, consider_words)\n",
    "    else:\n",
    "        def f(word):\n",
    "            return classify_word_as_specific_word(word,word_percentage_dict,mean_percentage)\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word[0].isupper():\n",
    "            if f(word):\n",
    "                res.append(word)\n",
    "                \n",
    "    return set(res)\n",
    "    \n",
    "\n",
    "def classify_word_as_specific_word(word, word_dict, threshold, consider_words = None):\n",
    "    \n",
    "    if consider_words is not None:\n",
    "        for w in consider_words:\n",
    "            if w in word:\n",
    "                return True\n",
    "    \n",
    "    if len(word) < 4:\n",
    "        return False\n",
    "    \n",
    "    if word in word_dict:\n",
    "        if word_dict[word] < threshold:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb723cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_word_as_specific_word(\"Klimaschutz\",word_percentage_dict,mean_percentage,consider_words=[\"Klima\",\"Treibhaus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7aa4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_word_as_specific_word(\"Klimaschutz\",word_percentage_dict,mean_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set_climate = get_specific_words(text, \n",
    "                                      word_percentage_dict, \n",
    "                                      mean_percentage, \n",
    "                                      consider_words=[\"Klima\",\"Treibhaus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055dae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_climate = find_all_documents_containing_words_in_corpus(invert_index, word_set_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5e4fb",
   "metadata": {},
   "source": [
    "898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee56996",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_climate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(documents_climate[1].items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e363e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(documents_climate[0])[201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60980c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_from_documents(documents):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for entry in documents[0]:\n",
    "        res.append(entry[0])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_climate = get_indices_from_documents(documents_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd599e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate_speeches = df_preprocessed.iloc[indices_climate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0523e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate_speeches[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ce401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preprocessed[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate_speeches[\"speaker\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d08f0",
   "metadata": {},
   "source": [
    "Now the sentiment analysis is executed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
